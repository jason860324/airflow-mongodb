
# -*- coding: utf-8 -*-
# Author        : Jesse Wei
# LastUpdate    : 2020/10/04
# Impact        : Jobs generated by SQLG
# Message       : Humanity towards others, we live by sharing. Fear can hold you prisoner, only hope can set you free.

# from __future__ import print_function
import logging
import re
import airflow
import pendulum
from datetime import datetime, timedelta
from airflow.sensors.external_task import ExternalTaskSensor

from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.contrib.sensors.file_sensor import FileSensor
from airflow import models
from airflow.models import Variable, DagModel, DagBag
from airflow.operators.python_operator import BranchPythonOperator
from airflow.operators.dummy_operator import DummyOperator


# For ODP platform
# from acme.operators.sqlg_oracle import OracleOperatorWithTemplatedParams
# from airflow.operators.oracle_operator import OracleOperator

#from acme.operators.sqlg_mssql import MsSqlOperatorWithTemplatedParams
#from airflow.operators.mssql_operator import MsSqlOperator

# DB_NAME = 'DWH' # for future xDB operator

proj_start_date = pendulum.datetime(2021, 1, 1,  tz="Etc/GMT-8")
tmpl_search_path = Variable.get("sql_path")
data_stage_imp_ptn = '_ODS_'
data_stage = []

# list for standard internval order sequence
std_interval = {
    '@once' 		:1,
    '@hourly' 		:2,
    '0 5 * * *'		:3,
    '0 5 * * 0'		:4,
    '0 5 1 * *'		:5,
    '0 5 1 */3 *'	:6,
    '0 5 1 1 *'		:7,
}

# function to sync execution for diff frequency
def sqlg_exec_date_fn(dt, context):
    var_date = Variable.get("sqlg_execution_date")
    ti = context['ti']
    dag = context['dag']
    ti_exec_date = context['logical_date'] 
    schedule_interval = dag.schedule_interval
    
    # if wait INIT and standard freq then set as default {{ ds }} # set in planner
    # else use dag own execution date
    if ti.task.external_dag_id == 'D_STG_INIT' and schedule_interval[0] == '@':
        exec_date = pendulum.parse(var_date)
    else:        
        exec_date = ti_exec_date

    print("sqlg_exec_date_fn::DEBUG:external_dag_id, exec_date:", ti.task.external_dag_id, exec_date)
    return exec_date




args = {
    "owner": "IEC960923",
    'start_date': proj_start_date,
    'provide_context': True,
    "email": ["Chen.Ivy@inventec.com","tao-dhbdadmin@inventec.com","Yang.KimiYP@inventec.com"],
    'email_on_failure': True,
}	
# XSLT:loop: declaration: END}


# XSLT:loop: JOB_FLOW_NAME: START{
job_flow_name = "D_ODS_SAL"
data_stage = job_flow_name.split('_')
tags = data_stage
#args["email"] = ["Chen.Ivy@inventec.com","tao-dhbdadmin@inventec.com","Yang.KimiYP@inventec.com"]
D_ODS_SAL = airflow.DAG(
    "D_ODS_SAL",
    tags=tags, 
    schedule_interval="0 5 * * *",

    dagrun_timeout=timedelta(minutes=60*4),
    template_searchpath=tmpl_search_path,
    default_args=args,
    # start_date=proj_start_date,    
    max_active_runs=1
	)

# XSLT:loop: JOB_FLOW_NAME: END}


# JOB_TYPE=ODS-MAIN
my_taskid = "ODS_PSG_CUSTOMER"
ODS_PSG_CUSTOMER = BashOperator(
    #autocommit=True,
    task_id=my_taskid,
    #pool = "sql_pool",
    dag=D_ODS_SAL,

    # parameters=({":END_DT_CHAR":"{{ (execution_date.astimezone('Asia/Taipei')).strftime('%Y%m%d') }}"}),    
    #timeout=60*60*3,
    bash_command= "echo EXECUTE SQLEXT." + my_taskid + "_SP "+  
		"{{ logical_date.in_tz(dag.timezone).strftime('%Y%m%d')  }} /" +
		"{{ logical_date.in_timezone(dag.timezone).strftime('%Y%m%d')  }} /" +
		"{{ (logical_date.in_timezone(dag.timezone)).strftime('%Y%m%d')  }} /" +
        "{{ logical_date.in_timezone('Asia/Taipei').strftime('%Y%m%d')  }} /" +
		"{{ (logical_date.astimezone(dag.timezone)).strftime('%Y%m%d') }} /" +
		"{{ (dag_run.logical_date.astimezone(dag.timezone)).strftime('%Y%m%d') }}  /" +
		# "{{ (dag_run.logical_date.astimezone('Asia/Taipei')).strftime('%Y%m%d') }}  /" +
		"{{ (logical_date.strftime('%Y%m%d')) }}  /" +
        ";"
    )



#   Cross dag sensor
# 	XSLT:loop: JOB_FLOW_NAME-and-PRE_JOB: External:START{{

def branch_D_ODS_SALxD_STG_INIT__SYS_STS_STG(**context):
    mydag = context["dag"]
    dagbag = DagBag()
    upstream = dagbag.get_dag("D_STG_INIT")
			
    # print("branch::DEBUG:upstream.latest_execution_date:", upstream.latest_execution_date)
    # print("branch::DEBUG:mydag.execution_date:", context['execution_date'])
    up_sch_interval = std_interval.get(upstream.schedule_interval)
    my_sch_interval = std_interval.get(mydag.schedule_interval)

    if up_sch_interval is None or my_sch_interval is None:
        if (up_sch_interval is None and my_sch_interval is None) and (upstream.schedule_interval == mydag.schedule_interval):
            return ["proxy_D_ODS_SALxD_STG_INIT__SYS_STS_STG","D_ODS_SALxD_STG_INIT__SYS_STS_STG"]
    elif std_interval[upstream.schedule_interval] >= std_interval[mydag.schedule_interval]:
        if upstream.latest_execution_date == context["execution_date"]:
            return ["proxy_D_ODS_SALxD_STG_INIT__SYS_STS_STG","D_ODS_SALxD_STG_INIT__SYS_STS_STG"]
    return ["proxy_D_ODS_SALxD_STG_INIT__SYS_STS_STG"]


my_taskid = "BRANCH_D_ODS_SALxD_STG_INIT__SYS_STS_STG"
BRANCH_D_ODS_SALxD_STG_INIT__SYS_STS_STG= BranchPythonOperator(
    task_id=my_taskid,
    python_callable=branch_D_ODS_SALxD_STG_INIT__SYS_STS_STG,
    dag=D_ODS_SAL,
    provide_context=True,
)


my_taskid = "proxy_D_ODS_SALxD_STG_INIT__SYS_STS_STG"
proxy_D_ODS_SALxD_STG_INIT__SYS_STS_STG= DummyOperator(
    task_id=my_taskid,
    trigger_rule="none_failed_or_skipped",
    dag=D_ODS_SAL,
)


#   Cross dag sensor
			
my_taskid = "D_ODS_SALxD_STG_INIT__SYS_STS_STG"
D_ODS_SALxD_STG_INIT__SYS_STS_STG= ExternalTaskSensor(
    #pool = "sensor_pool",
    task_id=my_taskid,
    external_dag_id="D_STG_INIT",
    external_task_id="SYS_STS_STG",
    mode="reschedule",
    dag=D_ODS_SAL,
    check_existence=True,
    timeout=60*60*1,
    retries=5,
    retry_delay=timedelta(minutes=3),
    execution_date_fn=sqlg_exec_date_fn
)


BRANCH_D_ODS_SALxD_STG_INIT__SYS_STS_STG.set_downstream(proxy_D_ODS_SALxD_STG_INIT__SYS_STS_STG)

BRANCH_D_ODS_SALxD_STG_INIT__SYS_STS_STG.set_downstream(D_ODS_SALxD_STG_INIT__SYS_STS_STG)

D_ODS_SALxD_STG_INIT__SYS_STS_STG.set_downstream(proxy_D_ODS_SALxD_STG_INIT__SYS_STS_STG)	
# BRANCH_D_INT_SALxD_ODS_SAL__ODS_PSG_CUSTOMER.set_downstream(proxy_D_INT_SALxD_ODS_SAL__ODS_PSG_CUSTOMER)

#BRANCH_D_INT_SALxD_ODS_SAL__ODS_PSG_CUSTOMER.set_downstream(D_INT_SALxD_ODS_SAL__ODS_PSG_CUSTOMER)

#D_INT_SALxD_ODS_SAL__ODS_PSG_CUSTOMER.set_downstream(proxy_D_INT_SALxD_ODS_SAL__ODS_PSG_CUSTOMER)


# 	XSLT:loop: JOB_FLOW_NAME-and-PRE_JOB: External: END}}


# XSLT:loop: JOB_FLOW_NAME: START{

# 	XSLT:loop: Rows-by-JOB_FLOW_NAME: JOB_NAME: START{{
# 	 	FLOW: D_ODS_SAL.ODS_PSG_CUSTOMER
proxy_D_ODS_SALxD_STG_INIT__SYS_STS_STG.set_downstream(ODS_PSG_CUSTOMER)

